#!/usr/bin/env python3
"""
Multi-stage curriculum learning with ensemble training for ultra-optimized PINN
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
from ultra_enhanced_pinn import UltraEnhancedPINN, UltraDataProcessor
import time
from concurrent.futures import ThreadPoolExecutor
import copy

class CurriculumTrainer:
    """Multi-stage curriculum learning trainer"""
    
    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.Adam(model.parameters(), lr=0.001)
        self.criterion = nn.MSELoss()
        
        # Curriculum stages
        self.stages = [
            {
                'name': 'Gentle Dynamics',
                'epochs': 50,
                'physics_weight': 5.0,
                'reg_weight': 10.0,
                'direct_id_weight': 20.0,
                'data_filter': 'gentle',  # Filter for gentle maneuvers
                'lr': 0.001
            },
            {
                'name': 'Moderate Excitation', 
                'epochs': 75,
                'physics_weight': 15.0,
                'reg_weight': 15.0,
                'direct_id_weight': 40.0,
                'data_filter': 'moderate',  # Mix of gentle + moderate
                'lr': 0.0005
            },
            {
                'name': 'Aggressive Dynamics',
                'epochs': 100,
                'physics_weight': 25.0,
                'reg_weight': 20.0,
                'direct_id_weight': 60.0,
                'data_filter': 'aggressive',  # All data including aggressive
                'lr': 0.0003
            },
            {
                'name': 'Fine-tuning',
                'epochs': 50,
                'physics_weight': 30.0,
                'reg_weight': 25.0,
                'direct_id_weight': 80.0,
                'data_filter': 'all',  # All data
                'lr': 0.0001
            }
        ]
        
        self.stage_losses = []
        
    def filter_data_by_stage(self, df, stage_filter):
        """Filter data based on curriculum stage"""
        
        if stage_filter == 'gentle':
            # Only low angular rate data
            max_rate_threshold = 1.0  # rad/s
            mask = (df['p'].abs() < max_rate_threshold) & \
                   (df['q'].abs() < max_rate_threshold) & \
                   (df['r'].abs() < max_rate_threshold)
            return df[mask]
            
        elif stage_filter == 'moderate':
            # Low to moderate angular rates
            max_rate_threshold = 3.0  # rad/s
            mask = (df['p'].abs() < max_rate_threshold) & \
                   (df['q'].abs() < max_rate_threshold) & \
                   (df['r'].abs() < max_rate_threshold)
            return df[mask]
            
        elif stage_filter == 'aggressive':
            # Include high angular rate data
            max_rate_threshold = 6.0  # rad/s
            mask = (df['p'].abs() < max_rate_threshold) & \
                   (df['q'].abs() < max_rate_threshold) & \
                   (df['r'].abs() < max_rate_threshold)
            return df[mask]
            
        else:  # 'all'
            return df
    
    def train_stage(self, stage, train_loader, val_loader):
        """Train a single curriculum stage"""
        
        print(f"\nStage: {stage['name']}")
        print(f"Epochs: {stage['epochs']}, LR: {stage['lr']}")
        print(f"Weights - Physics: {stage['physics_weight']}, "
              f"Reg: {stage['reg_weight']}, DirectID: {stage['direct_id_weight']}")
        
        # Update learning rate
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = stage['lr']
        
        stage_losses = {
            'train': [], 'val': [], 'physics': [], 'reg': [], 'direct_id': []
        }
        
        for epoch in range(stage['epochs']):
            # Training
            self.model.train()
            total_loss = 0
            total_physics = 0
            total_reg = 0
            total_id = 0
            
            for data, target in train_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                self.optimizer.zero_grad()
                
                output = self.model(data)
                
                # Individual losses
                data_loss = self.criterion(output, target)
                physics_loss = self.model.complete_physics_loss(data, output, target)
                direct_id_loss = self.model.direct_parameter_identification_loss(data, target)
                reg_loss = self.model.comprehensive_parameter_regularization_loss()
                
                # Combined loss with stage weights
                loss = (0.2 * data_loss +
                       stage['physics_weight'] * physics_loss + 
                       stage['reg_weight'] * reg_loss +
                       stage['direct_id_weight'] * direct_id_loss)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                self.model.constrain_parameters()
                
                total_loss += loss.item()
                total_physics += physics_loss.item()
                total_reg += reg_loss.item()
                total_id += direct_id_loss.item()
            
            # Validation
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for data, target in val_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = self.model(data)
                    val_loss += self.criterion(output, target).item()
            
            # Record losses
            stage_losses['train'].append(total_loss / len(train_loader))
            stage_losses['val'].append(val_loss / len(val_loader))
            stage_losses['physics'].append(total_physics / len(train_loader))
            stage_losses['reg'].append(total_reg / len(train_loader))
            stage_losses['direct_id'].append(total_id / len(train_loader))
            
            if epoch % 20 == 0:
                print(f'  Epoch {epoch:03d}: Train={total_loss/len(train_loader):.2f}, '
                      f'Val={val_loss/len(val_loader):.6f}, '
                      f'Physics={total_physics/len(train_loader):.2f}')
        
        self.stage_losses.append(stage_losses)
        return stage_losses
    
    def curriculum_train(self, df):
        """Execute complete curriculum training"""
        
        print("MULTI-STAGE CURRICULUM LEARNING")
        print("=" * 50)
        
        processor = UltraDataProcessor()
        
        for i, stage in enumerate(self.stages):
            print(f"\n[STAGE {i+1}/{len(self.stages)}]")
            
            # Filter data for this stage
            stage_data = self.filter_data_by_stage(df, stage['data_filter'])
            print(f"Data points: {len(stage_data):,} (filtered from {len(df):,})")
            
            # Prepare data
            X, y = processor.prepare_sequences(stage_data)
            
            if len(X) == 0:
                print("No data for this stage, skipping...")
                continue
            
            # Split data
            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Scale data (fit on first stage, transform on others)
            if i == 0:
                X_train_scaled, y_train_scaled = processor.fit_transform(X_train, y_train)
            else:
                X_train_scaled, y_train_scaled = processor.transform(X_train, y_train)
            
            X_val_scaled, y_val_scaled = processor.transform(X_val, y_val)
            
            # Create loaders
            train_dataset = TensorDataset(torch.FloatTensor(X_train_scaled), 
                                        torch.FloatTensor(y_train_scaled))
            val_dataset = TensorDataset(torch.FloatTensor(X_val_scaled), 
                                      torch.FloatTensor(y_val_scaled))
            
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
            
            # Train this stage
            self.train_stage(stage, train_loader, val_loader)
            
            # Save checkpoint after each stage
            torch.save(self.model.state_dict(), f'models/curriculum_stage_{i+1}_model.pth')
            
        print(f"\nCurriculum learning completed!")
        return processor

class EnsembleTrainer:
    """Ensemble training with multiple models"""
    
    def __init__(self, num_models=10, device='cpu'):
        self.num_models = num_models
        self.device = device
        self.models = []
        self.trainers = []
        
    def create_ensemble(self):
        """Create ensemble of models with different initializations"""
        
        print(f"Creating ensemble of {self.num_models} models...")
        
        for i in range(self.num_models):
            # Create model with different random initialization
            torch.manual_seed(42 + i)  # Different seeds for diversity
            model = UltraEnhancedPINN(
                input_size=12, 
                hidden_size=256 + i*8,  # Slightly different architectures
                output_size=16, 
                num_layers=6
            )
            
            trainer = CurriculumTrainer(model, self.device)
            
            self.models.append(model)
            self.trainers.append(trainer)
            
        print(f"Ensemble created with {self.num_models} diverse models")
    
    def train_ensemble(self, df):
        """Train all models in ensemble"""
        
        print(f"\nTRAINING ENSEMBLE OF {self.num_models} MODELS")
        print("=" * 60)
        
        # Train models sequentially (to avoid memory issues)
        processors = []
        
        for i, trainer in enumerate(self.trainers):
            print(f"\n[ENSEMBLE MODEL {i+1}/{self.num_models}]")
            print(f"Architecture: {trainer.model.network}")
            
            processor = trainer.curriculum_train(df)
            processors.append(processor)
            
            # Save individual model
            torch.save(trainer.model.state_dict(), f'models/ensemble_model_{i+1}.pth')
            
        return processors
    
    def evaluate_ensemble(self, true_values):
        """Evaluate ensemble performance"""
        
        print(f"\nENSEMBLE EVALUATION")
        print("=" * 30)
        
        # Parameter predictions from each model
        all_predictions = {param: [] for param in true_values.keys()}
        
        for i, model in enumerate(self.models):
            learned_values = {
                'Mass': model.m.item(),
                'Jxx': model.Jxx.item(),
                'Jyy': model.Jyy.item(),
                'Jzz': model.Jzz.item(),
                'Gravity': model.g.item()
            }
            
            for param in true_values.keys():
                all_predictions[param].append(learned_values[param])
        
        # Ensemble predictions (mean)
        ensemble_predictions = {}
        ensemble_std = {}
        
        for param in true_values.keys():
            predictions = np.array(all_predictions[param])
            ensemble_predictions[param] = np.mean(predictions)
            ensemble_std[param] = np.std(predictions)
        
        # Calculate ensemble accuracy
        total_accuracy = 0
        print(f"{'Parameter':<12} {'True':<12} {'Ensemble':<12} {'Std':<10} {'Accuracy':<10}")
        print("-" * 70)
        
        for param in true_values.keys():
            true_val = true_values[param]
            pred_val = ensemble_predictions[param]
            std_val = ensemble_std[param]
            
            error_pct = abs(pred_val - true_val) / true_val * 100
            accuracy_pct = max(0, 100 - error_pct)
            total_accuracy += accuracy_pct
            
            if param == 'Mass':
                print(f"{param:<12} {true_val:<12.6f} {pred_val:<12.6f} {std_val:<10.4f} {accuracy_pct:<9.1f}%")
            elif param == 'Gravity':
                print(f"{param:<12} {true_val:<12.3f} {pred_val:<12.3f} {std_val:<10.4f} {accuracy_pct:<9.1f}%")
            else:
                print(f"{param:<12} {true_val:<12.2e} {pred_val:<12.2e} {std_val:<10.2e} {accuracy_pct:<9.1f}%")
        
        avg_accuracy = total_accuracy / len(true_values)
        print("-" * 70)
        print(f"{'ENSEMBLE':<12} {'':<12} {'':<12} {'':<10} {avg_accuracy:<9.1f}%")
        
        return avg_accuracy, ensemble_predictions, ensemble_std

def run_ultra_optimized_training():
    """Run complete ultra-optimized training pipeline"""
    
    print("ULTRA-OPTIMIZED PINN TRAINING PIPELINE")
    print("=" * 60)
    print("Features:")
    print("âœ“ Aggressive aerobatic training data (32.6x higher excitation)")
    print("âœ“ Complete motor dynamics + aerodynamics + gyroscopic + ground effect")
    print("âœ“ Multi-stage curriculum learning (4 stages)")
    print("âœ“ Ensemble learning (10 models)")
    print("âœ“ 13 learnable physical parameters")
    print("âœ“ Advanced regularization and constraints")
    
    # Load aggressive training data
    print(f"\nLoading aggressive training data...")
    df = pd.read_csv('results/aggressive_quadrotor_training_data.csv')
    print(f"Loaded {len(df):,} data points with max angular rate {df[['p','q','r']].abs().max().max():.1f} rad/s")
    
    # Create and train ensemble
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    ensemble = EnsembleTrainer(num_models=3, device=device)  # Reduced to 3 for speed
    ensemble.create_ensemble()
    
    start_time = time.time()
    processors = ensemble.train_ensemble(df)
    training_time = time.time() - start_time
    
    print(f"\nTotal training time: {training_time/60:.1f} minutes")
    
    # Evaluate ensemble
    true_values = {
        'Mass': 0.068,
        'Jxx': 6.86e-5,
        'Jyy': 9.2e-5,
        'Jzz': 1.366e-4,
        'Gravity': 9.81
    }
    
    final_accuracy, predictions, uncertainties = ensemble.evaluate_ensemble(true_values)
    
    print(f"\nFINAL ULTRA-OPTIMIZED RESULTS:")
    print(f"Ensemble accuracy: {final_accuracy:.1f}%")
    print(f"Improvement from original 6.7%: {final_accuracy/6.7:.1f}x better")
    print(f"Improvement from enhanced 78.4%: +{final_accuracy-78.4:.1f}%")
    
    if final_accuracy > 85.0:
        print(f"\nðŸŽ‰ SUCCESS! Achieved >85% accuracy target!")
    elif final_accuracy > 78.4:
        print(f"\nâœ… IMPROVED! Beat previous best of 78.4%")
    else:
        print(f"\nðŸ“ˆ PROGRESS: Advanced techniques implemented")
    
    # Save best ensemble model
    best_idx = 0  # Could select best performing model
    torch.save(ensemble.models[best_idx].state_dict(), 'models/ultra_optimized_best_model.pth')
    
    return final_accuracy, predictions, uncertainties

if __name__ == "__main__":
    accuracy, predictions, uncertainties = run_ultra_optimized_training()