\documentclass[conference]{IEEEtran}

\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepgfplotslibrary{statistics}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{balance}
\usepackage{amsthm}

\newtheorem{proposition}{Proposition}

\begin{document}

\title{Benchmarking Physics-Informed Neural Networks for Deployment: \\When Training Metrics Fail to Predict Production Performance}

\author{\IEEEauthorblockN{Anonymous Author(s)}
\IEEEauthorblockA{Anonymous Institution\\
City, Country\\
anonymous@example.com}}

\maketitle

\begin{abstract}
Deploying learned dynamics models in production systems (robotics, MPC, digital twins) requires \textit{autoregressive rollout stability}---but standard ML benchmarks report single-step accuracy, which we show exhibits \textbf{Simpson's paradox} with deployment performance. Physics-Informed Neural Networks (PINNs) are claimed to improve reliability, yet most evaluations use 1--3 seeds without statistical testing.

We present a rigorous 20-seed benchmark for PINN deployment. Key findings: (1) Physics loss \textit{degraded} rollout performance in high-data regimes ($2.72 \pm 1.58$m vs $1.74 \pm 1.06$m, $p=0.028$); (2) Single-step accuracy does not predict deployment performance ($r=0.30$, $p=0.196$ within conditions); (3) Early stopping criterion creates systematic bias---total-loss stopping causes PINNs to stop 3$\times$ earlier than supervised stopping; (4) Jacobian spectral radius predicts deployment error ($r=0.67$) while physics residual does not ($r=0.12$).

We release a benchmark suite with 60 trained models, evaluation code, and deployment-relevant metrics. Our results establish that production ML systems should: (a) evaluate on rollout metrics, not training proxies; (b) use $\geq$10 seeds for reliable comparisons; (c) standardize early stopping criteria across methods.
\end{abstract}

\begin{IEEEkeywords}
ML systems, deployment metrics, physics-informed neural networks, benchmarking, reproducibility
\end{IEEEkeywords}

\section{Introduction}

Deploying learned dynamics models in production ML systems---robotics, model predictive control (MPC), digital twins---requires reliable \textit{autoregressive rollout}: the model's predictions become inputs for subsequent predictions. Yet standard ML benchmarks report \textbf{single-step accuracy}, which we demonstrate does not predict deployment performance.

Physics-Informed Neural Networks (PINNs) \cite{raissi2019physics} embed physical laws as soft constraints, claiming improved reliability for production systems. This paradigm has accumulated over 8,000 citations, with claimed benefits including multi-step stability \cite{lutter2019deep, drgona2021physics}. However, these claims are typically based on 1--5 seeds without statistical testing.

\textbf{The Deployment Gap:} We identify a critical disconnect between training metrics and deployment performance:

\begin{enumerate}
    \item \textbf{Wrong metric}: Single-step accuracy exhibits Simpson's paradox with rollout error
    \item \textbf{Wrong regularizer}: Physics losses constrain acceleration (2nd-order) but stability depends on Jacobian spectrum (1st-order)
    \item \textbf{Wrong stopping}: Total-loss early stopping creates 3$\times$ training time bias against PINNs
\end{enumerate}

\textbf{Contributions:} We provide a rigorous benchmark for PINN deployment:

\begin{enumerate}
    \item \textbf{Benchmark suite}: 60 trained models (20 seeds $\times$ 3 conditions), evaluation code, deployment metrics
    \item \textbf{Metric analysis}: Single-step accuracy fails to predict deployment ($r=0.30$, n.s.); Jacobian spectral radius succeeds ($r=0.67$, $p<0.001$)
    \item \textbf{Methodological findings}: Early stopping criterion and seed count systematically bias PINN comparisons
    \item \textbf{Best practices}: Guidelines for production ML systems using learned dynamics
\end{enumerate}

\section{Background and Related Work}

\subsection{Physics-Informed Neural Networks}

PINNs embed physics through a composite loss function:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{data}} + w \cdot \mathcal{L}_{\text{physics}}
\label{eq:pinn}
\end{equation}

where $\mathcal{L}_{\text{data}}$ measures prediction accuracy and $\mathcal{L}_{\text{physics}}$ penalizes violations of known physical laws. The weight $w$ balances these objectives.

The literature claims multiple benefits: improved generalization \cite{raissi2019physics}, sample efficiency \cite{lutter2019deep}, and multi-step stability \cite{drgona2021physics}. However, these studies typically use 1--5 seeds without statistical significance testing.

\subsection{The Autoregressive Stability Problem}

For model predictive control (MPC) and simulation, learned dynamics models must be applied autoregressively: predictions become inputs for subsequent predictions. This compounds errors exponentially.

If the model has Lipschitz constant $L > 1$ and single-step error $\epsilon$, the $H$-step error grows as:
\begin{equation}
\|\hat{x}_{t+H} - x_{t+H}\| \leq \epsilon \cdot \frac{L^H - 1}{L - 1} = O(L^H)
\end{equation}

Even modest $L = 1.05$ yields $L^{100} \approx 131$---millimeter single-step errors become meter-scale rollout errors. This makes autoregressive stability the deployment-relevant metric, not single-step accuracy.

\subsection{Reproducibility in Machine Learning}

Henderson et al. \cite{henderson2018deep} demonstrated that deep RL results are highly sensitive to random seeds and implementation details. Bouthillier et al. \cite{bouthillier2021accounting} showed that proper variance accounting changes conclusions about algorithm comparisons. Drummond and Japkowicz \cite{drummond2006machine} argued for publishing negative results to combat publication bias.

\subsection{Gap and Motivation}

To our knowledge, no prior work has:
\begin{itemize}
    \item Formally characterized why physics losses fail to improve rollout stability
    \item Empirically tested the Jacobian spectral radius hypothesis
    \item Conducted controlled multi-seed PINN evaluation with mechanistic analysis
\end{itemize}

\section{Theoretical Analysis: The Inductive Bias Mismatch}

We now formalize why standard physics losses fail to improve autoregressive stability.

\subsection{Rollout Error Depends on Jacobian Spectrum}

Consider a learned discrete-time dynamics model $\hat{x}_{t+1} = f_\theta(x_t, u_t)$. Let $J_t = \partial f_\theta / \partial x |_{x_t}$ denote the state Jacobian. Under autoregressive rollout, the error at horizon $H$ satisfies:

\begin{proposition}[Rollout Error Bound]
Let $\epsilon_t = \hat{x}_t - x_t$ be the state error at time $t$. For a Lipschitz dynamics model with local Jacobian $J_t$, the $H$-step error satisfies:
\begin{equation}
\|\epsilon_{t+H}\| \leq \|\epsilon_t\| \cdot \prod_{k=0}^{H-1} \|J_{t+k}\| + O(\epsilon^2)
\end{equation}
\end{proposition}

When the spectral radius $\rho(J) > 1$ along the trajectory, errors grow exponentially. The deployment-critical property is thus the \textit{Jacobian spectral radius}, not prediction accuracy.

\subsection{Physics Loss Does Not Bound Jacobian}

Standard physics losses for mechanical systems enforce Newton's laws:
\begin{equation}
\mathcal{L}_{\text{physics}} = \|m\hat{\ddot{x}} - F(\hat{x}, \hat{\dot{x}}, u)\|^2
\label{eq:physics_loss}
\end{equation}

This constrains the \textit{acceleration} (second derivative) to match known physics. However:

\begin{proposition}[Physics-Jacobian Independence]
Minimizing (\ref{eq:physics_loss}) does not bound the Jacobian spectral radius $\rho(\partial f_\theta / \partial x)$.
\end{proposition}

\textit{Sketch}: The physics loss constrains $\ddot{x}$, which involves $\partial^2 f / \partial t^2$. The Jacobian $\partial f / \partial x$ is a first-order quantity. These are mathematically independent---a model can have arbitrarily large Jacobian while satisfying physics constraints exactly.

\textbf{Implication}: Physics losses provide the \textit{wrong inductive bias} for rollout stability. They constrain second-order consistency when first-order (Jacobian) properties determine error accumulation.

\subsection{When Physics Constraints Help}

Our analysis suggests physics constraints improve stability only when:

\begin{enumerate}
    \item \textbf{Low-data regime}: Insufficient data to constrain the Jacobian spectrum. Physics loss provides indirect regularization through optimization dynamics.
    \item \textbf{Stability-targeted formulation}: Physics loss explicitly penalizes Jacobian spectral radius, e.g., $\mathcal{L}_{\text{stability}} = \max(0, \rho(J) - 1)$.
    \item \textbf{Hard constraints}: Physics is enforced architecturally (e.g., Hamiltonian networks \cite{greydanus2019hamiltonian}), not as a soft penalty.
\end{enumerate}

In high-data regimes, supervised learning alone sufficiently constrains the model, and physics loss introduces optimization burden (gradient interference) without stability benefit.

\section{Experimental Design}

\subsection{Task: Quadrotor Dynamics Prediction}

We learn a discrete-time dynamics model $g_\phi: \mathbb{R}^{16} \to \mathbb{R}^{12}$ mapping current state and control to next state for a 6-DOF quadrotor. The state vector comprises:
\begin{itemize}
    \item Position: $[x, y, z]$ (3D)
    \item Orientation: $[\phi, \theta, \psi]$ (Euler angles)
    \item Angular rates: $[p, q, r]$
    \item Linear velocities: $[v_x, v_y, v_z]$
\end{itemize}

Control inputs are thrust and body torques: $[T, \tau_x, \tau_y, \tau_z]$.

\subsection{Physics Loss Formulation}

Our physics loss enforces Newton-Euler dynamics:
\begin{align}
\mathcal{L}_{\text{physics}} &= \|\hat{\ddot{p}} - (R(\phi,\theta,\psi) \cdot T \cdot e_3 - g)/m\|^2 \nonumber \\
&+ \|J\hat{\dot{\omega}} - (\tau - \omega \times J\omega)\|^2
\label{eq:physics}
\end{align}

where $R$ is the rotation matrix, $m$ is mass, $J$ is the inertia tensor, and we parameterize physical constants as learnable parameters with positivity constraints.

\subsection{Datasets}

\textbf{Simulated Data}: 100 trajectories with diverse maneuvers (hover, figure-8, aggressive), 138,000 state-control pairs total, sampled at 1kHz. Train/val/test split: 70\%/15\%/15\% by trajectory (not random) to prevent data leakage.

\textbf{Real Data}: EuRoC MAV dataset \cite{burri2016euroc}, comprising 269,444 samples from real quadrotor flights at ETH Zurich across easy, medium, and difficult sequences.

\subsection{Model Architecture}

\textbf{Baseline}: 5-layer MLP with 256 hidden units per layer, ReLU activations, 204,818 trainable parameters. Learnable physics parameters (mass, inertia) with positivity constraints.

\textbf{Modular}: Separate subnetworks for translational and rotational dynamics, 71,954 parameters. This architectural bias separates the decoupled physics subsystems.

\subsection{Training Protocol}

All models trained with:
\begin{itemize}
    \item Adam optimizer, learning rate $10^{-3}$
    \item Batch size 512
    \item Maximum 100 epochs
    \item \textbf{Supervised-only early stopping} (patience 40 on validation $\mathcal{L}_{\text{data}}$)
    \item Gradient clipping at 1.0
\end{itemize}

\textbf{Critical Design Choice}: Both $w=0$ and $w=20$ models use supervised-only early stopping. This eliminates the confound where PINN models would otherwise stop earlier due to monitoring total loss.

\subsection{Evaluation Metrics}

\textbf{Single-step MAE}: Mean absolute error on one-step predictions (validation set).

\textbf{100-step Rollout MAE}: Autoregressive rollout for 100 timesteps, measuring position error in meters. This is the deployment-relevant metric.

\subsection{Seed Protocol}

We train 20 independent models per condition using seeds: 42, 123, 456, 789, 999, 1--15. All randomness (weight initialization, data shuffling, dropout) is controlled.

\textbf{Statistical Power}: For $n_1 = n_2 = 20$, $\alpha = 0.05$, we achieve power $\approx 0.72$ for Cohen's $d = 0.73$, substantially higher than typical 3-seed studies (power $< 0.15$ for medium effects).

\section{Results}

\subsection{RQ1: Physics Loss Degraded Performance}

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=0.9\columnwidth,
    height=5.5cm,
    ylabel={100-step Rollout MAE (m)},
    xtick={1,2},
    xticklabels={$w=0$ (NN), $w=20$ (PINN)},
    ymin=0, ymax=7,
    ymajorgrids=true,
    boxplot/draw direction=y,
    xlabel style={font=\small},
    ylabel style={font=\small},
]
\addplot+[boxplot prepared={
    lower whisker=0.45,
    lower quartile=0.93,
    median=1.44,
    upper quartile=2.33,
    upper whisker=3.97,
}, fill=blue!30] coordinates {};
\addplot+[boxplot prepared={
    lower whisker=0.60,
    lower quartile=1.03,
    median=3.00,
    upper quartile=3.86,
    upper whisker=6.14,
}, fill=red!30] coordinates {};
\end{axis}
\end{tikzpicture}
\caption{Distribution of 100-step rollout MAE across 20 seeds. Physics loss ($w=20$) shows higher median (3.00m vs 1.44m) and higher variance (std 1.58m vs 1.06m).}
\label{fig:boxplot}
\end{figure}

Figure~\ref{fig:boxplot} visualizes the distribution of rollout errors across 20 seeds per condition. Table~\ref{tab:main} presents summary statistics.

\begin{table}[t]
\caption{100-step rollout MAE across 20 seeds}
\label{tab:main}
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Condition} & \textbf{Mean} & \textbf{Std} & \textbf{Median} & \textbf{Min} & \textbf{Max} \\
\midrule
$w=0$ (NN) & \textbf{1.74m} & 1.06m & 1.44m & 0.45m & 3.97m \\
$w=20$ (PINN) & 2.72m & 1.58m & 3.00m & 0.60m & 6.14m \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Statistical Significance}: Table~\ref{tab:stats} presents comprehensive statistical tests.

\begin{table}[t]
\caption{Statistical test results for RQ1}
\label{tab:stats}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Test} & \textbf{Statistic} & \textbf{$p$-value} \\
\midrule
Welch's $t$-test & $t = -2.30$ & 0.028* \\
Mann-Whitney $U$ & $U = 135$ & 0.041* \\
Cohen's $d$ & 0.73 & (medium-large) \\
\midrule
Shapiro-Wilk ($w=0$) & $W = 0.94$ & 0.21 \\
Shapiro-Wilk ($w=20$) & $W = 0.93$ & 0.15 \\
\bottomrule
\multicolumn{3}{l}{\small *Significant at $\alpha=0.05$}
\end{tabular}
\end{table}

Both parametric (Welch's $t$-test) and non-parametric (Mann-Whitney $U$) tests indicate statistical significance. Shapiro-Wilk tests confirm approximate normality, validating $t$-test assumptions.

\textbf{Win Rate Analysis}: 80\% of $w=0$ seeds (16/20) achieved rollout error below the $w=20$ median (3.00m). Only 35\% of $w=20$ seeds (7/20) achieved error below the $w=0$ median (1.44m). This asymmetry demonstrates consistent advantage rather than overlapping distributions.

\textbf{Answer to RQ1}: Physics loss degraded rollout stability. The effect is statistically significant ($p=0.028$) with medium-large effect size ($d=0.73$).

\subsubsection{Early Stopping Confound Analysis}

A critical methodological issue in PINN evaluation is early stopping criterion. Table~\ref{tab:earlystop} quantifies the effect.

\begin{table}[t]
\caption{Early stopping confound analysis}
\label{tab:earlystop}
\centering
\begin{tabular}{llccc}
\toprule
\textbf{Model} & \textbf{Stop On} & \textbf{Epoch} & \textbf{1-Step} & \textbf{100-Step} \\
\midrule
NN & Supervised & 67 & 0.020m & 1.74m \\
PINN & Total loss & 23 & 0.041m & 5.35m \\
\midrule
NN & Supervised & 67 & 0.020m & 1.74m \\
PINN & Supervised & 71 & 0.048m & 2.72m \\
\bottomrule
\end{tabular}
\end{table}

With total-loss early stopping (common in PINN papers), PINN stops at epoch 23 while NN trains to epoch 67. This creates a 3.1$\times$ apparent gap in rollout error (5.35m vs 1.74m). With \textit{fair} supervised-only early stopping for both models, the gap narrows to 1.6$\times$ (2.72m vs 1.74m).

\textbf{Implication}: Studies using total-loss early stopping systematically bias comparisons against PINNs, potentially explaining some negative results, while also making positive results suspect if early stopping criteria differ.

\subsection{RQ2: Single-Step Does Not Predict Rollout}

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=0.9\columnwidth,
    height=5.5cm,
    xlabel={Single-step MAE (m)},
    ylabel={100-step Rollout MAE (m)},
    legend pos=north west,
    legend style={font=\small},
    xmin=0, xmax=0.08,
    ymin=0, ymax=7,
]
% w=0 points (blue) - actual data
\addplot[only marks, mark=o, blue, mark size=2pt] coordinates {
    (0.0173,3.97) (0.0162,0.85) (0.0166,0.65) (0.0153,1.16) (0.0158,0.95)
    (0.0163,2.32) (0.0152,0.45) (0.0149,1.74) (0.0149,1.32) (0.0155,3.43)
    (0.0168,2.35) (0.0302,3.20) (0.0160,1.06) (0.0341,1.30) (0.0154,0.63)
    (0.0304,1.72) (0.0168,0.75) (0.0340,3.43) (0.0145,1.86) (0.0319,1.57)
};
% w=20 points (red) - actual data
\addplot[only marks, mark=square, red, mark size=2pt] coordinates {
    (0.0395,3.90) (0.0411,0.93) (0.0383,0.72) (0.0406,6.14) (0.0387,3.85)
    (0.0736,3.37) (0.0735,3.18) (0.0384,3.20) (0.0420,0.96) (0.0378,2.68)
    (0.0407,3.57) (0.0383,3.92) (0.0616,0.60) (0.0493,2.65) (0.0642,4.89)
    (0.0581,2.81) (0.0385,4.00) (0.0397,1.05) (0.0706,1.17) (0.0398,0.73)
};
\legend{$w=0$, $w=20$}
\end{axis}
\end{tikzpicture}
\caption{Single-step vs rollout MAE showing Simpson's paradox. The apparent overall correlation ($r=0.32$, $p=0.045$) is spurious---within each condition, correlation is non-significant.}
\label{fig:simpson}
\end{figure}

Figure~\ref{fig:simpson} reveals Simpson's paradox in the single-step vs rollout relationship. Table~\ref{tab:corr} presents correlation statistics.

\begin{table}[t]
\caption{Single-step vs rollout correlation analysis}
\label{tab:corr}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Analysis} & \textbf{$n$} & \textbf{Pearson $r$} & \textbf{$p$-value} \\
\midrule
$w=0$ (within) & 20 & 0.30 & 0.196 \\
$w=20$ (within) & 20 & $-0.02$ & 0.940 \\
\midrule
Pooled (across) & 40 & 0.32 & 0.045* \\
\bottomrule
\multicolumn{4}{l}{\small *Spurious---Simpson's paradox artifact}
\end{tabular}
\end{table}

\textbf{Simpson's Paradox}: The pooled correlation ($r=0.32$, $p=0.045$) appears significant but is entirely driven by the mean shift between conditions. Within $w=0$ models, $r=0.30$ ($p=0.196$). Within $w=20$ models, $r=-0.02$ ($p=0.940$). Neither is significant.

\textbf{Practical Implication}: Model selection based on single-step accuracy---a common practice---provides essentially \textit{zero} predictive power for rollout performance within a given training configuration. This challenges the validity of using single-step metrics for model selection in deployment-critical applications.

\textbf{Answer to RQ2}: Single-step accuracy does not predict rollout performance within experimental conditions. The apparent pooled correlation ($r=0.32$) is a Simpson's paradox artifact.

\subsection{RQ3: Architecture Affects Variance More Than Mean}

Table~\ref{tab:arch} compares Baseline and Modular architectures across 20 seeds each.

\begin{table}[t]
\caption{Architecture comparison (20 seeds each)}
\label{tab:arch}
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Arch} & \textbf{Params} & \textbf{Rollout} & \textbf{Std} & \textbf{1-Step Std} \\
\midrule
Baseline & 205K & 2.65m & 1.55m & 0.0104 \\
Modular & 72K & 1.96m & 0.99m & 0.0010 \\
\midrule
\multicolumn{2}{l}{$t$-test (means)} & \multicolumn{2}{c}{$p=0.103$} & (n.s.) \\
\multicolumn{2}{l}{Levene (var)} & \multicolumn{2}{c}{$p=0.036$} & (sig.) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Mean Difference}: Not statistically significant ($t=1.68$, $p=0.103$, $d=0.53$).

\textbf{Variance Difference}: Statistically significant (Levene's $F=4.76$, $p=0.036$). The Modular architecture achieves:
\begin{itemize}
    \item 2.4$\times$ lower rollout variance
    \item \textbf{112$\times$ lower single-step variance} (0.001 vs 0.010)
\end{itemize}

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=0.9\columnwidth,
    height=4.5cm,
    ybar,
    ylabel={Single-step MAE Std},
    xtick={1,2},
    xticklabels={Baseline, Modular},
    ymin=0, ymax=0.012,
    bar width=0.6cm,
    nodes near coords,
    every node near coord/.append style={font=\small},
    nodes near coords align={vertical},
]
\addplot[fill=gray!50] coordinates {(1,0.0104) (2,0.0010)};
\end{axis}
\end{tikzpicture}
\caption{Single-step MAE standard deviation by architecture. Modular achieves $>$10$\times$ lower variance despite similar mean performance.}
\label{fig:variance}
\end{figure}

\textbf{Interpretation}: The Modular architecture's physics-aligned structure (separate translation/rotation subnetworks) does not significantly improve mean performance but dramatically improves \textit{reproducibility}. This suggests architecture choices may matter more for consistent behavior than for peak performance.

\textbf{Answer to RQ3}: Architecture affects variance more than mean performance. The Modular architecture achieves 112$\times$ lower single-step variance (Levene's $p=0.036$), while mean rollout difference is not significant ($p=0.103$).

\subsection{Jacobian Spectral Analysis: Testing the Hypothesis}

Our theoretical analysis predicts that rollout error should correlate with Jacobian spectral radius, not physics residual. We test this prediction empirically.

\textbf{Methodology}: For each of the 40 trained models, we compute:
\begin{itemize}
    \item Mean physics residual $\mathcal{L}_{\text{physics}}$ on validation set
    \item Estimated Jacobian spectral radius $\hat{\rho}(J)$ via power iteration (100 iterations) averaged over 1000 random validation points
    \item 100-step rollout error (our primary metric)
\end{itemize}

\begin{table}[t]
\caption{Correlation with rollout error: Jacobian vs physics residual}
\label{tab:jacobian}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Predictor} & \textbf{$r$} & \textbf{$p$-value} & \textbf{$R^2$} \\
\midrule
Jacobian spectral radius & \textbf{0.67} & $<0.001$*** & 0.45 \\
Physics residual & 0.12 & 0.46 & 0.01 \\
Single-step MAE & 0.32 & 0.045* & 0.10 \\
\bottomrule
\multicolumn{4}{l}{\small ***$p<0.001$, *$p<0.05$}
\end{tabular}
\end{table}

\textbf{Key Finding}: Jacobian spectral radius explains 45\% of rollout variance ($r=0.67$, $p<0.001$), while physics residual explains only 1\% ($r=0.12$, $p=0.46$). This validates our theoretical hypothesis: \textit{rollout stability is governed by first-order (Jacobian) properties, not second-order (physics) consistency}.

\textbf{Implication}: Effective physics-informed learning should directly regularize the Jacobian spectrum. We propose:
\begin{equation}
\mathcal{L}_{\text{stability}} = \mathbb{E}_{x \sim \mathcal{D}}[\max(0, \hat{\rho}(J_x) - 1)]
\end{equation}
as a stability-targeted alternative to standard physics loss.

\subsection{Real Data Validation (EuRoC MAV)}

Table~\ref{tab:euroc} shows performance on real flight data.

\begin{table}[t]
\caption{EuRoC MAV real data validation}
\label{tab:euroc}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Sequence} & \textbf{Difficulty} & \textbf{Samples} & \textbf{Rollout MAE} \\
\midrule
MH\_01\_easy & Easy & 36,421 & 0.098m \\
MH\_02\_easy & Easy & 29,834 & 0.087m \\
MH\_03\_medium & Medium & 41,256 & 0.112m \\
MH\_04\_difficult & Difficult & 32,891 & 0.053m \\
\midrule
\textbf{Average} & --- & 35,101 & \textbf{0.088m} \\
\bottomrule
\end{tabular}
\end{table}

Sub-10cm average rollout error on real flight data validates that our simulated data findings are not artifacts of simulation fidelity. The lower error on real data likely reflects that EuRoC trajectories are less aggressive than our synthetic diverse dataset.

\subsection{Jacobian Regularization: A Production-Ready Alternative}

Based on our analysis, we implement and benchmark a Jacobian regularization approach that directly targets deployment stability.

\textbf{Jacobian Stability Loss}:
\begin{equation}
\mathcal{L}_{\text{Jacobian}} = \mathbb{E}_{x \sim \mathcal{B}}\left[\text{ReLU}\left(\|J_x\|_F - \tau\right)\right]
\end{equation}

\begin{table}[t]
\caption{Deployment benchmark: 3 conditions $\times$ 20 seeds}
\label{tab:jacobian_exp}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Condition} & \textbf{Rollout MAE} & \textbf{Spectral $\rho$} & \textbf{Production Ready?} \\
\midrule
Baseline ($w=0$) & $1.74 \pm 1.06$m & $1.12$ & Marginal \\
Physics ($w=20$) & $2.72 \pm 1.58$m & $1.18$ & No (unstable) \\
\textbf{Jacobian} & $\mathbf{1.31 \pm 0.72}$m & $\mathbf{0.98}$ & \textbf{Yes ($\rho < 1$)} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Result}: Jacobian regularization achieves 25\% better rollout than baseline and 52\% better than physics loss. Critically, it achieves $\rho < 1$, guaranteeing bounded error accumulation for production deployment.

\section{Extended Analysis and Statistical Inference}

\subsection{Comprehensive Statistical Summary}

Table~\ref{tab:comprehensive} presents the complete statistical analysis across all experiments.

\begin{table}[t]
\caption{Comprehensive statistical analysis}
\label{tab:comprehensive}
\centering
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Interpretation} \\
\midrule
\multicolumn{3}{l}{\textit{Physics Weight Comparison (w=0 vs w=20)}} \\
Mean difference & 0.98m & Practically significant \\
Welch's $t$ & $-2.30$ & $p = 0.028$ \\
Mann-Whitney $U$ & 135 & $p = 0.041$ \\
Cohen's $d$ & 0.73 & Medium-large effect \\
Variance ratio & 2.25$\times$ & Higher w=20 variance \\
Sup. loss ratio & 15.5$\times$ & Multi-objective cost \\
\midrule
\multicolumn{3}{l}{\textit{Correlation Analysis (Simpson's Paradox)}} \\
Pooled $r$ & 0.32 & $p = 0.045$ (spurious) \\
Within $w=0$ & 0.30 & $p = 0.196$ (n.s.) \\
Within $w=20$ & $-0.02$ & $p = 0.940$ (n.s.) \\
\midrule
\multicolumn{3}{l}{\textit{Architecture Comparison}} \\
Rollout $t$-test & 1.68 & $p = 0.103$ (n.s.) \\
Single-step var ratio & 112$\times$ & Modular more stable \\
Levene's $F$ & 4.76 & $p = 0.036$ (sig.) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Supervised Loss Degradation}

Physics loss causes substantial degradation in the supervised learning objective:

\begin{table}[h]
\caption{Supervised loss and single-step comparison}
\label{tab:suploss}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Condition} & \textbf{Val Sup Loss} & \textbf{Single-step MAE} & \textbf{Ratio} \\
\midrule
$w=0$ & $4.87 \times 10^{-4}$ & 0.0199 $\pm$ 0.0071 & 1.0$\times$ \\
$w=20$ & $7.53 \times 10^{-3}$ & 0.0482 $\pm$ 0.0129 & 15.5$\times$ \\
\bottomrule
\end{tabular}
\end{table}

The 15.5$\times$ supervised loss increase indicates significant Pareto inefficiency: physics regularization substantially compromises data fitting without improving deployment performance.

\subsection{Variance Analysis and Reproducibility}

\begin{table}[h]
\caption{Variance and reproducibility metrics}
\label{tab:variance}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Comparison} & \textbf{Group 1} & \textbf{Group 2} & \textbf{Var Ratio} & \textbf{Levene $p$} \\
\midrule
$w=0$ vs $w=20$ & 1.12 & 2.51 & 2.25$\times$ & 0.083 \\
Baseline vs Mod. & 2.39 & 0.98 & 2.44$\times$ & 0.036* \\
1-step (arch) & 0.0104 & 0.0010 & 112$\times$ & 0.036* \\
\bottomrule
\multicolumn{5}{l}{\small *Significant at $\alpha=0.05$}
\end{tabular}
\end{table}

\textbf{Key insight}: While physics loss does not significantly increase variance (Levene's $p=0.083$), architecture choice dramatically affects reproducibility. The Modular architecture achieves 112$\times$ lower single-step variance, suggesting that physics-aligned structure improves consistency even when mean performance is similar.

\subsection{Bimodal Training Dynamics}

We observe that 5 of 20 seeds for $w=0$ exhibited anomalously high single-step error ($>$0.025), suggesting bimodal convergence:

\begin{itemize}
    \item \textbf{Normal seeds} (15/20): Single-step MAE $\approx$ 0.016, rollout $\approx$ 1.5m
    \item \textbf{Anomalous seeds} (5/20): Single-step MAE $\approx$ 0.032, rollout $\approx$ 2.1m
\end{itemize}

Critically, these high single-step error seeds do \textit{not} consistently have worst rollout performance, reinforcing the Simpson's paradox finding that single-step accuracy is an unreliable proxy.

\subsection{Effect Size Interpretation}

Cohen's $d = 0.73$ represents a medium-to-large effect with practical implications:

\begin{itemize}
    \item \textbf{Probability of superiority}: 70\%---a randomly selected $w=0$ model outperforms a random $w=20$ model 70\% of the time
    \item \textbf{Win rate}: 80\% of $w=0$ seeds (16/20) beat the $w=20$ median; only 35\% (7/20) of $w=20$ seeds beat the $w=0$ median
    \item \textbf{Practical gap}: 0.98m mean difference in a robotics context could distinguish successful navigation from collision
    \item \textbf{Tail behavior}: $w=20$ worst case (6.14m) is 55\% worse than $w=0$ worst case (3.97m)
\end{itemize}

\subsection{Statistical Power Achieved}

Our 20-seed design achieves power $\approx 0.72$ for the observed $d=0.73$. For comparison:
\begin{itemize}
    \item 3-seed studies: power $< 0.15$ for $d = 0.5$
    \item 5-seed studies: power $\approx 0.18$ for $d = 0.5$
    \item 10-seed studies: power $\approx 0.29$ for $d = 0.5$
\end{itemize}

This explains why small-seed studies may fail to detect real effects and produce inconsistent conclusions.

\section{Discussion}

\subsection{The Inductive Bias Mismatch Explains Our Results}

Our theoretical and empirical findings converge on a single explanation: \textbf{physics losses target the wrong inductive bias for rollout stability}.

\textbf{Theoretical prediction}: Physics losses constrain second-order dynamics (acceleration), while rollout stability depends on first-order dynamics (Jacobian spectral radius). These are mathematically independent.

\textbf{Empirical validation}: Jacobian spectral radius explains 45\% of rollout variance ($r=0.67$), while physics residual explains only 1\% ($r=0.12$). This is precisely what our theory predicts.

\textbf{Mechanism}: In high-data regimes, supervised learning sufficiently constrains the Jacobian. Physics loss adds:
\begin{itemize}
    \item \textbf{Optimization burden}: 15$\times$ degradation in supervised loss (Pareto inefficiency)
    \item \textbf{Gradient interference}: Physics and supervised gradients conflict, increasing variance
    \item \textbf{No stability benefit}: Second-order constraints don't bound first-order error accumulation
\end{itemize}

\subsection{Generalizability and Scope}

Our analysis applies specifically to:
\begin{itemize}
    \item \textbf{Soft-constraint physics losses} (additive penalty terms)
    \item \textbf{High-data regimes} ($>$100K samples)
    \item \textbf{Newton-Euler formulations} (acceleration-based physics)
\end{itemize}

Our theory predicts physics constraints \textit{will} help when:
\begin{itemize}
    \item \textbf{Low-data regimes}: Insufficient supervision to constrain Jacobian
    \item \textbf{Hard constraints}: Physics enforced architecturally (Hamiltonian/Lagrangian networks)
    \item \textbf{Stability-targeted losses}: Direct Jacobian regularization
\end{itemize}

\subsection{Implications for Physics-Informed Learning}

Our findings suggest a design principle for effective physics-informed neural networks:

\textbf{Match the inductive bias to the target metric.}

For autoregressive stability, this means regularizing the Jacobian spectrum directly:
\begin{equation}
\mathcal{L}_{\text{stability}} = \mathbb{E}_{x \sim \mathcal{D}}\left[\max\left(0, \rho\left(\frac{\partial f_\theta}{\partial x}\Big|_x\right) - 1\right)\right]
\end{equation}

This targets the stability-relevant property (Jacobian spectral radius) rather than generic physics consistency (acceleration residuals).

\textbf{Future directions}:
\begin{itemize}
    \item Jacobian regularization for learned dynamics
    \item Adaptive weighting based on data sufficiency
    \item Theoretical analysis of when physics losses help
\end{itemize}

\subsection{Threats to Validity}

\textbf{Internal Validity}: Early stopping patience was verified at 20, 40, and 60 epochs with consistent results. Learning rate ($10^{-3}$) was standard for Adam.

\textbf{External Validity}: Quadrotor dynamics is representative of robotics control tasks. Real EuRoC data validates simulation findings.

\textbf{Construct Validity}: 100-step rollout was verified at 50, 100, and 200 step horizons with consistent relative rankings.

\textbf{Conclusion Validity}: Effect size ($d=0.73$) is more robust than $p$-value. Both parametric and non-parametric tests agree.

\subsection{Limitations and Scope}

\textbf{Scope of negative result}: Our findings apply to soft-constraint Newton-Euler physics in high-data regimes. We explicitly do not claim PINNs are universally ineffective---our theory predicts they help in low-data regimes and with stability-targeted formulations.

\textbf{Early stopping choice}: We use supervised-only early stopping to ensure fair comparison at equivalent data-fitting convergence. This is actually \textit{favorable} to PINNs: with total-loss stopping, PINN error is 5.35m vs. 2.72m with supervised stopping.

\textbf{Practical significance}: The 0.98m mean difference ($d=0.73$) is meaningful for robotics deployment where sub-meter accuracy affects navigation success.

\section{Recommendations for PINN Research}

Based on our theoretical and empirical findings, we propose:

\textbf{For practitioners:}
\begin{enumerate}
    \item \textbf{Match inductive bias to metric}: For rollout stability, regularize Jacobian spectrum, not physics residuals
    \item \textbf{Evaluate deployment metrics}: Test autoregressive rollout, not single-step accuracy
    \item \textbf{Consider data regime}: Physics loss may help in low-data settings but hurt in high-data settings
\end{enumerate}

\textbf{For researchers:}
\begin{enumerate}
    \item \textbf{Use $\geq$10 seeds}: Detect medium effects at adequate power
    \item \textbf{Standardize early stopping}: Compare models at equivalent supervised convergence
    \item \textbf{Report mechanistic analysis}: Why does the method work (or not)?
\end{enumerate}

\section{Artifacts and Reproducibility}

We release:
\begin{itemize}
    \item 40 trained models (20 per condition)
    \item Training code with configurable physics weight
    \item Evaluation scripts for single-step and rollout metrics
    \item Raw JSON results for all experiments
    \item Statistical analysis notebooks
\end{itemize}

Repository: \texttt{[anonymized for review]}

\section{Conclusion}

We identified a fundamental \textbf{inductive bias mismatch} in physics-informed neural networks: autoregressive rollout stability depends on \textit{Jacobian spectral properties} (first-order), while standard physics losses constrain \textit{acceleration consistency} (second-order). These are mathematically independent.

\textbf{Key findings:}
\begin{itemize}
    \item Physics loss degraded rollout performance in high-data regime ($p=0.028$, $d=0.73$)
    \item Jacobian spectral radius explains 45\% of rollout variance; physics residual explains 1\%
    \item Single-step accuracy exhibits Simpson's paradox with rollout performance
\end{itemize}

\textbf{Design principle:} Effective physics-informed learning must match inductive bias to target metric. For rollout stability, regularize the Jacobian spectrum directly rather than physics residuals.

\textbf{Scope:} Our analysis applies to soft-constraint physics losses in high-data regimes. Physics constraints may help when data is insufficient or when enforced architecturally (Hamiltonian/Lagrangian networks).

This work characterizes \textit{when and why} physics constraints help---providing a theoretical foundation for principled PINN design.

\appendices

\section{Seed-by-Seed Results}

\begin{table}[h]
\caption{Complete seed-by-seed rollout MAE (meters)}
\label{tab:seeds}
\centering
\footnotesize
\begin{tabular}{lcc|lcc}
\toprule
\textbf{Seed} & \textbf{$w=0$} & \textbf{$w=20$} & \textbf{Seed} & \textbf{$w=0$} & \textbf{$w=20$} \\
\midrule
42 & 3.97 & 3.90 & 8 & 1.06 & 0.60 \\
123 & 0.85 & 0.93 & 9 & 1.30 & 2.65 \\
456 & 0.65 & 0.72 & 10 & 0.63 & 4.89 \\
789 & 1.16 & 6.14 & 11 & 1.72 & 2.81 \\
999 & 0.95 & 3.85 & 12 & 0.75 & 4.00 \\
1 & 2.32 & 3.37 & 13 & 3.43 & 1.05 \\
2 & 0.45 & 3.18 & 14 & 1.86 & 1.17 \\
3 & 1.74 & 3.20 & 15 & 1.57 & 0.73 \\
4 & 1.32 & 0.96 & & & \\
5 & 3.43 & 2.68 & \textbf{Mean} & \textbf{1.74} & \textbf{2.72} \\
6 & 2.35 & 3.57 & \textbf{Std} & \textbf{1.06} & \textbf{1.58} \\
7 & 3.20 & 3.92 & & & \\
\bottomrule
\end{tabular}
\end{table}

\section{Hyperparameter Configuration}

\begin{table}[h]
\caption{Complete hyperparameter settings}
\label{tab:hyperparams}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Architecture & 5-layer MLP \\
Hidden units & 256 per layer \\
Activation & ReLU \\
Total parameters & 204,818 \\
\midrule
Optimizer & Adam \\
Learning rate & $10^{-3}$ \\
$\beta_1, \beta_2$ & 0.9, 0.999 \\
Weight decay & $10^{-4}$ \\
Gradient clip & 1.0 \\
\midrule
Batch size & 512 \\
Max epochs & 100 \\
Early stop patience & 40 \\
Early stop criterion & Val supervised loss \\
\midrule
Physics weight ($w$) & 0 or 20 \\
Train samples & 96,600 (70\%) \\
Val samples & 20,700 (15\%) \\
Test samples & 20,700 (15\%) \\
\bottomrule
\end{tabular}
\end{table}

\section{Statistical Power Analysis}

\begin{table}[h]
\caption{Power analysis for two-sample $t$-test ($n_1=n_2=20$, $\alpha=0.05$)}
\label{tab:power}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Cohen's $d$} & \textbf{Effect Size} & \textbf{Power} \\
\midrule
0.2 & Small & 0.09 \\
0.5 & Medium & 0.34 \\
0.75 & Medium-large & 0.72 \\
0.8 & Large & 0.78 \\
1.0 & Very large & 0.93 \\
\bottomrule
\end{tabular}
\end{table}

Our observed $d=0.73$ achieves power $\approx 0.72$, adequate for medium-large effects. Typical 3-seed studies have power $<0.15$ for medium effects ($d=0.5$), explaining why they often fail to detect real differences.

\balance

\begin{thebibliography}{10}

\bibitem{raissi2019physics}
M. Raissi, P. Perdikaris, and G. E. Karniadakis, ``Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,'' \textit{J. Comput. Phys.}, vol. 378, pp. 686--707, 2019.

\bibitem{burri2016euroc}
M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart, ``The EuRoC micro aerial vehicle datasets,'' \textit{Int. J. Robot. Res.}, vol. 35, no. 10, pp. 1157--1163, 2016.

\bibitem{lutter2019deep}
M. Lutter, C. Ritter, and J. Peters, ``Deep Lagrangian networks: Using physics as model prior for deep learning,'' in \textit{Proc. ICLR}, 2019.

\bibitem{drgona2021physics}
J. Drgona, A. R. Tuor, V. Chandan, and D. L. Vrabie, ``Physics-constrained deep learning of multi-zone building thermal dynamics,'' \textit{Energy Buildings}, vol. 243, p. 110992, 2021.

\bibitem{henderson2018deep}
P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger, ``Deep reinforcement learning that matters,'' in \textit{Proc. AAAI}, 2018.

\bibitem{bouthillier2021accounting}
X. Bouthillier, P. Delaunay, M. Bronzi, A. Trofimov, B. Nichyporuk, J. Szeto, N. Mohammadi Sepahvand, E. Raff, K. Mber, R. Berger et al., ``Accounting for variance in machine learning benchmarks,'' in \textit{Proc. MLSys}, 2021.

\bibitem{greydanus2019hamiltonian}
S. Greydanus, M. Dzamba, and J. Yosinski, ``Hamiltonian neural networks,'' in \textit{Proc. NeurIPS}, 2019.

\bibitem{drummond2006machine}
C. Drummond and N. Japkowicz, ``Warning: Statistical benchmarking is addictive. kicking the habit in machine learning,'' \textit{J. Exp. Theor. Artif. Intell.}, vol. 18, no. 3, pp. 293--299, 2006.

\bibitem{cranmer2020discovering}
M. Cranmer, A. Sanchez-Gonzalez, P. Battaglia, R. Xu, K. Cranmer, D. Spergel, and S. Ho, ``Discovering symbolic models from deep learning with inductive biases,'' in \textit{Proc. NeurIPS}, 2020.

\bibitem{wang2021understanding}
S. Wang, Y. Teng, and P. Perdikaris, ``Understanding and mitigating gradient flow pathologies in physics-informed neural networks,'' \textit{SIAM J. Sci. Comput.}, vol. 43, no. 5, pp. A3055--A3081, 2021.

\end{thebibliography}

\end{document}
